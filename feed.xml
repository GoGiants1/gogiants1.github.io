<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gogiants1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gogiants1.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-15T07:11:45+00:00</updated><id>https://gogiants1.github.io/feed.xml</id><title type="html">Hyungwook Choi</title><subtitle>The personal website of Hyungwook Choi. </subtitle><entry><title type="html">[Paper Review] Presel: Pre-Instruction Data Selection for Visual Instruction Tuning</title><link href="https://gogiants1.github.io/blog/2025/Presel/" rel="alternate" type="text/html" title="[Paper Review] Presel: Pre-Instruction Data Selection for Visual Instruction Tuning"/><published>2025-06-15T00:00:00+00:00</published><updated>2025-06-15T00:00:00+00:00</updated><id>https://gogiants1.github.io/blog/2025/Presel</id><content type="html" xml:base="https://gogiants1.github.io/blog/2025/Presel/"><![CDATA[<h1 id="presel-english">Presel (English)</h1> <p><img src="../assets/img/presel.png" alt="Overview"/></p> <h2 id="-overview-of-presel-pre-instruction-data-selection-for-vit">ğŸ” Overview of PreSel: Pre-Instruction Data Selection for VIT</h2> <table> <thead> <tr> <th>Step</th> <th>Purpose</th> <th>Input/Output</th> <th>Core Idea</th> </tr> </thead> <tbody> <tr> <td><strong>1. Task-Importance Estimation</strong></td> <td>Decide <em>how much to sample</em> from each task</td> <td>Input: 5% reference set $D_{\text{ref}}$ (with instructions) â†’ Output: task weights $w(T_i)$</td> <td>(i) Fine-tune LVLM on small set â†’ Reference model <br/> (ii) Compute <strong>IRS</strong> (Instruction Relevance Score) to assess task importance</td> </tr> <tr> <td><strong>2. Task-wise Cluster-based Selection</strong></td> <td>Decide <em>which images to sample</em> from each task</td> <td>Input: Remaining 95% images (no instructions) + $w(T_i)$</td> <td>(i) Extract features using DINOv2 <br/> (ii) k-means clustering per task <br/> (iii) Select representative images via <strong>NC</strong> (Neighbor Centrality)</td> </tr> <tr> <td><strong>3. Instruction Generation &amp; LVLM Fine-tuning</strong></td> <td>Generate instructions only for selected subset $\mathcal{D}_S$ and fine-tune</td> <td>Output: (Image, Instruction) pairs â†’ LVLM tuning</td> <td>Save cost by generating instructions only for $\mathcal{D}_S \ll D$</td> </tr> </tbody> </table> <hr/> <h2 id="1-problem-formulation">1. Problem Formulation</h2> <p>Let:</p> \[D = \bigcup_{i=1}^{M} T_i\] <p>Where:</p> <ul> <li>$T_i$: set of unlabeled images for the $i$-th vision task (e.g., VQA, OCR)</li> <li>$D$: entire image pool</li> <li>Each image $I\in T_i$ is mapped to instruction $Y = F_i(I)$, a high-cost generation process (e.g., GPT or human annotator)</li> </ul> <h3 id="objective">Objective</h3> <p>Select a small subset $\mathcal{D}_S \subset D$ with $\mathcal{D}_S \ll D$, generate instructions only for these, and fine-tune the LVLM on the resulting pairs $(I_a, Y_a)$ to achieve near full-data performance.</p> <hr/> <h2 id="2-task-importance-estimation">2. Task-Importance Estimation</h2> <h3 id="21-reference-model">2.1 Reference Model</h3> <p>Randomly sample 5% of $D$ as $D_{\text{ref}}$ (with instruction $Y = (Q, R)$) and fine-tune LVLM for one epoch â†’ reference model.</p> <h3 id="22-instruction-relevance-score-irs">2.2 Instruction Relevance Score (IRS)</h3> <p>Given instruction $Y = (Q, R)$, define:</p> \[\mathcal{L}_{R|Q,I} = -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|} \log P_{\theta}(t^{R}_{j} \mid I, Q, t^{R}_{&lt;j}) \tag{1}\] \[\mathcal{L}_{R|I} = -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|} \log P_{\theta}(t^{R}_{j} \mid I, t^{R}_{&lt;j}) \tag{2}\] \[\text{IRS}(I, Y) = \frac{\mathcal{L}_{R|Q,I}}{\mathcal{L}_{R|I}} \tag{3}\] <ul> <li>A <strong>lower</strong> IRS implies $Q$ significantly aids in generating $R$ â†’ more important task.</li> <li>A <strong>higher</strong> IRS implies $Q$ does <em>not</em> help much â†’ less important task.</li> </ul> <h3 id="23-compute-task-importance">2.3 Compute Task Importance</h3> <p>Average IRS for task $T_i$:</p> \[s(T_i) = \frac{1}{|D_{\text{ref}}^i|} \sum_{I \in T_i} \text{IRS}(I, Y) \tag{4}\] <p>Compute task weight via softmax:</p> \[w(T_i) = \frac{\exp(-s(T_i)/\tau)}{\sum_{j=1}^{M} \exp(-s(T_j)/\tau)},\quad \tau = \frac{1}{\sqrt{M}} \tag{5}\] <hr/> <h2 id="3-task-wise-cluster-based-selection">3. Task-wise Cluster-based Selection</h2> <h3 id="31-visual-feature-extraction-and-clustering">3.1 Visual Feature Extraction and Clustering</h3> <p>For all unlabeled images $I \in T_i$, extract visual features $\mathbf{v}_I$ using DINOv2 ([CLS] token), and cluster into:</p> \[\left\{ A_c^i \right\}_{c=1}^{C},\quad \text{where } C = \frac{|T_i|}{100}\] <h3 id="32-cluster-allocation">3.2 Cluster Allocation</h3> <p>For cluster $A_c^i$, compute number of samples to pick:</p> \[n_c = \left\lfloor \frac{w(T_i) \cdot |A_c^i|}{|T_i|} \cdot |\mathcal{D}_S| \right\rfloor \tag{6}\] <h3 id="33-intra-cluster-selection-neighbor-centrality-nc">3.3 Intra-Cluster Selection: Neighbor Centrality (NC)</h3> <p>Score for each image $I$ based on cosine similarity with $k$-nearest neighbors:</p> \[s_{\text{NC}}(I) = \frac{1}{k} \sum_{I_a \in \text{kNN}(I)} \text{sim}(\mathbf{v}_I, \mathbf{v}_{I_a}) \tag{7}\] <ul> <li>Higher $s_{\text{NC}}(I)$ means image is <strong>central</strong> and <strong>representative</strong>.</li> </ul> <hr/> <h2 id="4-final-assembly-and-fine-tuning">4. Final Assembly and Fine-tuning</h2> <ul> <li>Selected images across all tasks form $\mathcal{D}_S$.</li> <li>Generate instructions <strong>only for $\mathcal{D}_S$</strong>.</li> <li>Fine-tune LVLM using $(I, Y)$ pairs from $\mathcal{D}_S$.</li> </ul> <hr/> <h2 id="5-key-takeaways">5. Key Takeaways</h2> <table> <thead> <tr> <th>Insight</th> <th>Benefit</th> </tr> </thead> <tbody> <tr> <td><strong>Instruction-free selection phase</strong></td> <td>Reduces GPT or annotation cost drastically</td> </tr> <tr> <td><strong>IRS for task relevance</strong></td> <td>Captures both redundancy and informativeness</td> </tr> <tr> <td><strong>Visual feature + NC</strong></td> <td>Enables language-free, representative image selection</td> </tr> <tr> <td><strong>Lightweight pipeline</strong></td> <td>Efficient for large-scale unlabeled datasets</td> </tr> </tbody> </table> <p>This approach enables scalable, cost-effective, and high-performance data curation for visual instruction tuning in Multimodal LLMs.</p> <h1 id="presel-ì •ë¦¬-korean">Presel ì •ë¦¬ (Korean)</h1> <table> <thead> <tr> <th>ë‹¨ê³„</th> <th>ëª©ì </th> <th>ì…ë ¥/ì¶œë ¥</th> <th>í•µì‹¬ ì•„ì´ë””ì–´</th> </tr> </thead> <tbody> <tr> <td><strong>1. Task-Importance Estimation</strong></td> <td>â€œì–´ë–¤ íƒœìŠ¤í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë§ì´ ë½‘ì„ê¹Œ?â€ ê²°ì •</td> <td>5 % ì°¸ì¡°ì§‘í•© $D_{\text{ref}}$ (ì´ë¯¸ì§€Â·ì§€ì‹œ í¬í•¨) â†’ ê°€ì¤‘ì¹˜ $w(T_i)$</td> <td>(i) LVLMì„ 1-epoch ì†ŒëŸ‰ íŒŒì¸íŠœë‹ â†’ Reference model <br/>(ii) <strong>IRS</strong>(Instruction Relevance Score)ë¡œ íƒœìŠ¤í¬ ì¤‘ìš”ë„ ì¸¡ì •</td> </tr> <tr> <td><strong>2. Task-wise Cluster-based Selection</strong></td> <td>ê° íƒœìŠ¤í¬ ë‚´ë¶€ì—ì„œ â€œë¬´ì—‡ì„ ë½‘ì„ê¹Œ?â€ ê²°ì •</td> <td>ë‚˜ë¨¸ì§€ 95 % ë¯¸ë¼ë²¨ ë°ì´í„° (ì´ë¯¸ì§€ë§Œ) + $w(T_i)$</td> <td>(i) DINOv2 íŠ¹ì„± ì¶”ì¶œ â†’ k-means êµ°ì§‘í™” <br/>(ii) êµ°ì§‘ í¬ê¸°Â·$w(T_i)$ ê¸°ë°˜ ìƒ˜í”Œ ìˆ˜ $n_c$ ì‚°ì • <br/>(iii) <strong>NC</strong>(Neighbor Centrality)ë¡œ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ</td> </tr> <tr> <td><strong>3. Instruction Generation &amp; LVLM Fine-tune</strong></td> <td>ìµœì¢… ì†Œê·œëª¨ ë°ì´í„° $\mathcal{D}_S$ì— ëŒ€í•´ ì§€ì‹œ ìƒì„±Â·íŒŒì¸íŠœë‹</td> <td>$\mathcal{D}_S$ (ì´ë¯¸ì§€) â†’ (ì´ë¯¸ì§€, ì§€ì‹œ) â†’ íŒŒì¸íŠœë‹</td> <td>ë¹„ìš© ì ˆê°: ì „ì²´ ëŒ€ì‹  ($\mathcal{D}_S \ll D$) ë§Œ instruction ìƒì„±</td> </tr> </tbody> </table> <hr/> <h2 id="1-ë¬¸ì œ-ì •ì˜-problem-formulation">1. ë¬¸ì œ ì •ì˜ (Problem Formulation)</h2> <ul> <li> <p><strong>í’€(pool) êµ¬ì„±</strong></p> \[D \;=\; \bigcup_{i=1}^{M} T_i,\qquad |D| = \text{ì´ ì´ë¯¸ì§€ ìˆ˜}\] <p>$T_i$ : VQA, OCR ë“± <strong>ì‹œê° íƒœìŠ¤í¬</strong>ì— ì†í•˜ëŠ” <em>unlabeled</em> ì´ë¯¸ì§€ ì§‘í•©.</p> </li> <li> <p><strong>ì§€ì‹œ ìƒì„± ë¹„ìš©</strong>: ê° $T_i$ì—ëŠ” GPT API í˜¸ì¶œÂ·ì‚¬ëŒ ë¼ë²¨ ë“± <strong>ê³ ë¹„ìš© ì ˆì°¨</strong> $F_i(\cdot)$ ì´ í•„ìš”.</p> </li> <li> <p><strong>ëª©í‘œ</strong>: $\mathcal{D}_S\subset D,\;|\mathcal{D}_S|!\ll!|D|$ ë¥¼ ë½‘ì•„ ì§€ì‹œë¥¼ ìƒì„±í•˜ê³  (ì´ë¯¸ì§€, ì§€ì‹œ) í˜ì–´ë¡œ LVLMì„ íŒŒì¸íŠœë‹ â†’ <strong>ì „ì²´ íŒŒì¸íŠœë‹ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥</strong> ë‹¬ì„±.</p> </li> </ul> <hr/> <h2 id="2-task-importance-estimation-1">2. Task-Importance Estimation</h2> <h3 id="2-1-ì°¸ì¡°-ëª¨ë¸-êµ¬ì¶•">2-1. ì°¸ì¡° ëª¨ë¸ êµ¬ì¶•</h3> <ul> <li>ë¬´ì‘ìœ„ <strong>5 %</strong> ì°¸ì¡°ì§‘í•© $D_{\text{ref}}$ì„ ì„ íƒ, 1 epoch íŒŒì¸íŠœë‹ â†’ <strong>Reference LVLM</strong>.</li> </ul> <h3 id="2-2-instruction-relevance-score-irs">2-2. Instruction Relevance Score (IRS)</h3> <p>ê° ìƒ˜í”Œ $(I,Q,R)$ ì— ëŒ€í•´</p> \[\mathcal{L}_{R|Q,I}= -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|}\!\log P_{\theta}\!\bigl(t^{R}_{j}\mid I,Q,t^{R}_{&lt;j}\bigr) \tag{1}\] \[\mathcal{L}_{R|I}= -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|}\!\log P_{\theta}\!\bigl(t^{R}_{j}\mid I,t^{R}_{&lt;j}\bigr) \tag{2}\] \[\textbf{IRS}(I,Y)=\frac{\mathcal{L}_{R|Q,I}}{\mathcal{L}_{R|I}} \tag{3}\] <ul> <li> <p><strong>í•´ì„</strong></p> <ul> <li>IRSâ†‘ â‡’ ì§ˆë¬¸ $Q$ê°€ <em>ë„ì›€ì´ ì•ˆ ë¨</em> â†’ íƒœìŠ¤í¬ ì¤‘ìš”ë„â†“</li> <li>IRSâ†“ â‡’ $Q$ ë•ë¶„ì— í˜¼ë™â†“ â†’ íƒœìŠ¤í¬ ì¤‘ìš”ë„â†‘</li> </ul> </li> </ul> <h3 id="2-3-íƒœìŠ¤í¬ë³„-í‰ê· -irsì™€-ê°€ì¤‘ì¹˜">2-3. íƒœìŠ¤í¬ë³„ í‰ê·  IRSì™€ ê°€ì¤‘ì¹˜</h3> \[s(T_i)=\frac{1}{|D^{i}_{\text{ref}}|}\sum_{I\in T_i}\text{IRS}(I,Y) \tag{4}\] \[w(T_i)=\frac{\exp\!\bigl(-s(T_i)/\tau\bigr)}{\sum_{j=1}^{M}\exp\!\bigl(-s(T_j)/\tau\bigr)}, \qquad \tau=\frac{1}{\sqrt{M}} \tag{5}\] <ul> <li>$w(T_i)$ : <strong>ìµœì¢… ìƒ˜í”Œ ë¹„ì¤‘</strong> (íƒœìŠ¤í¬ ì¤‘ìš”ë„ì— softmax ì ìš©).</li> </ul> <hr/> <h2 id="3-task-wise-cluster-based-selection-1">3. Task-wise Cluster-based Selection</h2> <h3 id="3-1-ì‹œê°-íŠ¹ì„±--êµ°ì§‘">3-1. ì‹œê° íŠ¹ì„± &amp; êµ°ì§‘</h3> <ul> <li>ëª¨ë“  unlabeled $I\in T_i$ ì— ëŒ€í•´ DINOv2 [30]ì˜ $[\text{CLS}]$ ë²¡í„° $\mathbf{v}_{I}$ ì¶”ì¶œ.</li> <li>$k$-means êµ°ì§‘: $C=\tfrac{|T_i|}{100}$ ê°œ â†’ êµ°ì§‘ $A^{i}_{c}$ ($c=1,\dots,C$) í˜•ì„±.</li> </ul> <h3 id="3-2-êµ°ì§‘-ìƒ˜í”Œ-ìˆ˜-ê²°ì •">3-2. êµ°ì§‘ ìƒ˜í”Œ ìˆ˜ ê²°ì •</h3> \[n_c=\Bigl\lfloor \frac{w(T_i)\,|A^{i}_{c}|}{|T_i|}\,|\mathcal{D}_S| \Bigr\rfloor \tag{6}\] <h3 id="3-3-intra-cluster-ëŒ€í‘œì„±--neighbor-centrality">3-3. Intra-Cluster ëŒ€í‘œì„± â€” Neighbor Centrality</h3> \[s_{\text{NC}}(I)=\frac{1}{k}\sum_{I_a\in k\text{NN}(I)}\! \operatorname{sim}\!\bigl(\mathbf{v}_{I},\mathbf{v}_{I_a}\bigr) \tag{7}\] <ul> <li>knn ì´ì›ƒ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„.</li> <li>$s_{\text{NC}}$ ë†’ì„ìˆ˜ë¡ <strong>êµ°ì§‘ ì¤‘ì‹¬</strong> â†’ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì„ íƒ.</li> </ul> <hr/> <h2 id="4-ì „ì²´-íŒŒì´í”„ë¼ì¸-figure-3-í•´ì„¤">4. ì „ì²´ íŒŒì´í”„ë¼ì¸ (Figure 3 í•´ì„¤)</h2> <ol> <li> <p><strong>ì¢Œì¸¡</strong>: ë‹¤íƒœìŠ¤í¬ ì´ë¯¸ì§€ í’€ $D$ â†’ 5 % ì¶”ì¶œí•´ $D_{\text{ref}}$ (ë…¹ìƒ‰)</p> <ul> <li>ì´ ë‹¨ê³„ì—ì„œë§Œ ì§€ì‹œ ìƒì„±Â·Reference LVLM í•™ìŠµ â†’ $w(T_i)$ ë„ì¶œ.</li> </ul> </li> <li> <p><strong>ì¤‘ì•™</strong>: ë‚˜ë¨¸ì§€ 95 % ëŠ” DINOv2 íŠ¹ì„± â†’ íƒœìŠ¤í¬ë³„ k-means â†’ êµ°ì§‘ ìƒ‰ìƒ(ì—°ë‘/ê°ˆìƒ‰â€¦).</p> </li> <li> <p><strong>ìš°ì¸¡</strong>: ê° êµ°ì§‘ ë‚´ NC ìƒìœ„ $n_c$ ì´ë¯¸ì§€ ì„ ì • â†’ $\mathcal{D}_S$ (íšŒìƒ‰ ìƒì).</p> </li> <li> <p><strong>í•˜ë‹¨ í™•ëŒ€</strong>: IRS ê³„ì‚° ê³¼ì •</p> <ul> <li> <table> <tbody> <tr> <td>In/Out í† í° ì‹œí€€ìŠ¤ ë¹„êµë¡œ $\mathcal{L}_{R</td> <td>I}$, $\mathcal{L}_{R</td> <td>Q,I}$ ì‚°ì¶œ.</td> </tr> </tbody> </table> </li> <li>íƒœìŠ¤í¬ë³„ í‰ê·  â†’ $w(T_i)$ ì‚°ì • í›„ ìœ—ë‹¨ê³„ë¡œ í”¼ë“œë°±.</li> </ul> </li> <li> <p><strong>ìµœì¢…</strong>: ì„ íƒëœ ì´ë¯¸ì§€ì—ë§Œ ì§€ì‹œ ìƒì„± â†’ LVLM ë‹¤ì‹œ íŒŒì¸íŠœë‹.</p> </li> </ol> <hr/> <h2 id="5-í•µì‹¬-í†µì°°--ì¥ì ">5. í•µì‹¬ í†µì°° &amp; ì¥ì </h2> <table> <thead> <tr> <th>í¬ì¸íŠ¸</th> <th>ì´ìœ /íš¨ê³¼</th> </tr> </thead> <tbody> <tr> <td><strong>â€œì§€ì‹œ ì—†ëŠ”â€ ìƒíƒœì—ì„œ <em>ì„ íƒ</em> â†’ ì§€ì‹œ ìƒì„±</strong></td> <td>GPT API ë¹„ìš©Â·íœ´ë¨¼ ë¼ë²¨ë§ ë¹„ìš© ëŒ€í­ ì ˆê°</td> </tr> <tr> <td>IRS ê¸°ë°˜ <strong>íƒœìŠ¤í¬ ì¤‘ìš”ë„</strong></td> <td>ì¤‘ë³µÂ·í•™ìŠµ ë‚œì´ë„ê¹Œì§€ ë°˜ì˜í•´ <em>ê· í˜• ì¡íŒ</em> ì„œë¸Œì…‹ êµ¬ì„±</td> </tr> <tr> <td><strong>ì‹œê° íŠ¹ì„±+NC</strong></td> <td>ì–¸ì–´ ì •ë³´ ì—†ì´ë„ <em>ëŒ€í‘œì„±</em> ì¤‘ì‹¬ ìƒ˜í”Œë§ â†’ íš¨ìœ¨ì  ì¼ë°˜í™”</td> </tr> <tr> <td>ê²½ëŸ‰ íŒŒì´í”„ë¼ì¸ (DINOv2, k-means)</td> <td>GPU ë©”ëª¨ë¦¬ / ì—°ì‚° ë¶€ë‹´ ìµœì†Œí™”, ëŒ€ê·œëª¨ í’€ì—ë„ ì ìš© ìš©ì´</td> </tr> </tbody> </table> <p>ìœ„ ê³¼ì •ì„ êµ¬í˜„í•˜ë©´ ì „ì²´ VIT ë°ì´í„°ì˜ <strong>5â€“10 %</strong> ë§Œìœ¼ë¡œë„ í’€ë°ì´í„° íŒŒì¸íŠœë‹ ìˆ˜ì¤€ì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œ, ì§€ì‹œ ìƒì„±-í•™ìŠµ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>]]></content><author><name></name></author><category term="deep-learning"/><category term="mllm"/><category term="datasets"/><category term="paper-review"/><summary type="html"><![CDATA[Presel (English)]]></summary></entry><entry><title type="html">[DiT] Scalable Diffusion Models with Transformers â€” Dev In Seoul</title><link href="https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers-dev-in-seoul/" rel="alternate" type="text/html" title="[DiT] Scalable Diffusion Models with Transformers â€” Dev In Seoul"/><published>2024-07-15T00:00:00+00:00</published><updated>2024-07-15T00:00:00+00:00</updated><id>https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers--dev-in-seoul</id><content type="html" xml:base="https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers-dev-in-seoul/"><![CDATA[<p>MathJax = { tex: {inlineMath: [[â€™$â€™, â€˜$â€™], [â€™\(â€˜, â€˜\)â€™]]} };</p> <p>ë…¼ë¬¸ <Scalable Diffusion="" Models="" with="" Transformers="">ì˜ ë‚´ìš©ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.ë…¼ë¬¸ ì¶œì²˜: ì•„ì¹´ì´ë¸Œ, Codeì´ ë…¼ë¬¸ì€ Facebook Research(í˜„ Meta AI)ì—ì„œ ê³µê°œí•œ ë…¼ë¬¸ì´ë©°, ì €ì ì¤‘ William PeeblesëŠ” OpenAIì—ì„œ Research Scientistë¡œ ì¼í•˜ê³  ìˆìœ¼ë©°, ì˜ìƒ ìƒì„± AI Soraì˜ ê°œë°œì„ ê³µë™ìœ¼ë¡œ ë¦¬ë“œí•˜ê³  ìˆë‹¤ê³  í•œë‹¤.DiT ì•„í‚¤í…ì²˜ì— ê´€í•œ ê´€ì‹¬ì€ OpenAIì˜ Sora ê³µê°œ ì´í›„ ëœ¨ê±°ì›Œì§„ ê²ƒ ê°™ìœ¼ë©°, ìµœê·¼ì— ê³µê°œëœ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì¸ Stable Diffusion 3, PixArt ê³„ì—´ì˜ ëª¨ë¸ì—ë„ ì ìš©ë˜ì–´ ë”ìš± ê°ê´‘ë°›ê³  ìˆë‹¤ê³  ìƒê°í•œë‹¤.ì´ ì—°êµ¬ëŠ” ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ì‚° ëª¨ë¸ì„ ì œì•ˆí•œë‹¤. ê¸°ì¡´ì˜ U-Net êµ¬ì¡°ë¥¼ ëŒ€ì²´í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬, í™•ì¥ì„±ê³¼ ì„±ëŠ¥ ë©´ì—ì„œ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ë ¤ í•œë‹¤.ìµœê·¼ ë”¥ëŸ¬ë‹ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ íšê¸°ì ì¸ ë°œì „ì„ ì´ë£¨ì—ˆìœ¼ë©°, íŠ¹íˆ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ê·¸ ê°€ëŠ¥ì„±ì„ ë„ë¦¬ ì…ì¦í•˜ê³  ìˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¯¸ì§€ ìƒì„± ë¶„ì•¼ì—ì„œ í™•ì‚° ëª¨ë¸ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì „í™˜í•˜ëŠ” ìƒˆë¡œìš´ ì‹œë„ë¥¼ ì†Œê°œí•œë‹¤. ì´ëŸ¬í•œ ì „í™˜ì„ í†µí•´ ëª¨ë¸ì˜ í™•ì¥ì„±ê³¼ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒí•  ìˆ˜ ìˆë‹¤.íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì£¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼(NLP)ì—ì„œ ì‹œì‘ë˜ì—ˆìœ¼ë‚˜, ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì‹œê° ì¸ì‹ê³¼ ê°™ì€ ë‹¤ë¥¸ ì˜ì—­ì—ì„œë„ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê²Œ ë˜ì—ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ìš” ê°•ì ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ìš°ìˆ˜í•œ í•™ìŠµ ëŠ¥ë ¥ê³¼ ë”ë¶ˆì–´ ë†’ì€ í™•ì¥ì„±ì´ë‹¤.í™•ì‚° ëª¨ë¸ì€ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ê°•ë ¥í•œ ë°©ë²•ìœ¼ë¡œ ìë¦¬ ì¡ì•˜ë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì£¼ë¡œ ì»¨ë³¼ë£¨ì…”ë„ U-Net ì•„í‚¤í…ì²˜(Latent Diffusion Model, LDM)ì— ê¸°ë°˜í•˜ì—¬ ë°œì „ë˜ì–´ ì™”ì§€ë§Œ, ë³¸ ì—°êµ¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì„ ë›°ì–´ë„˜ê³ ì í•œë‹¤.í™•ì‚° í”„ë¡œì„¸ìŠ¤ëŠ” ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ê³ , ì´ë¥¼ ì—­ì „ì‹œí‚¤ëŠ” ê³¼ì •ì„ ëª¨ë¸ë§í•œë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ ê³¼ì •ì„ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ ìˆ˜í–‰í•œë‹¤.íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ í™•ì‚° ëª¨ë¸ì˜ í•µì‹¬ì€ ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³  ê° íŒ¨ì¹˜ë¥¼ ë…ë¦½ì ì¸ í† í°ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ í† í°ë“¤ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ í†µí•´ ì²˜ë¦¬ë˜ë©°, ìµœì¢…ì ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ì œê±°ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.DiT ëª¨ë¸ì€ ë‹¤ì–‘í•œ êµ¬ì„±ê³¼ íŒ¨ì¹˜ í¬ê¸°ë¥¼ ê°€ì§€ê³  ImageNet ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨ë˜ì—ˆë‹¤. ëª¨ë¸ ì„±ëŠ¥ì€ FID(FrÃ©chet Inception Distance)ë¥¼ í†µí•´ í‰ê°€ë˜ì—ˆë‹¤.ë…¼ë¬¸ì˜ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„</Scalable></p> <p>ë‹¤ì–‘í•œ êµ¬ì„±ì˜ DiT ëª¨ë¸ë“¤ì´ í›ˆë ¨ì„ ê±°ì¹˜ë©° FID ì ìˆ˜ê°€ ê°œì„ ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. FID(FrÃ©chet Inception Distance)ëŠ” ìƒì„±ëœ ì˜ìƒì´ë‚˜ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ”ë° ìì£¼ ì‚¬ìš©ëœë‹¤. ì¢Œì¸¡ì˜ ê·¸ë˜í”„ì—ì„œëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ë†’ì•„ì§ˆìˆ˜ë¡, FIDëŠ” ë‚®ì•„ì ¸ì„œ ìƒì„± í’ˆì§ˆì´ ì¢‹ì•„ì§„ë‹¤ëŠ” ê²ƒì„ ì„¤ëª…í•œë‹¤. ìš°ì¸¡ì˜ ê·¸ë¦¼ì€ ê¸°ì¡´ì˜ U-Net ê¸°ë°˜ì˜ Diffusion Modelê³¼ì˜ ì—°ì‚°ëŸ‰ì„ ë¹„êµí•˜ì—¬ íš¨ìœ¨ì ì´ë¼ëŠ” ê²ƒì„ ì£¼ì¥í•˜ëŠ” ê·¸ë˜í”„ì´ë‹¤. ì´ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ êµ¬ì¡°ê°€ ê¸°ì¡´ í™•ì‚° ëª¨ë¸ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤.ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ì‚° ëª¨ë¸ì€ ë†’ì€ í™•ì¥ì„±ê³¼ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•œë‹¤. ì•ìœ¼ë¡œ ì´ ëª¨ë¸ì„ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ íƒêµ¬í•  ì˜ˆì •ì´ë‹¤.ì´ ë…¼ë¬¸ì—ì„œëŠ” Diffusion ê¸°ë°˜ì˜ ìƒì„± ëª¨ë¸ë§ì—ì„œ, ê¸°ì¡´ U-Net ê¸°ë°˜ì˜ LDM ì™¸ì— ë¯¸ë˜ë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ì„ íƒìƒ‰í•´ ë³´ëŠ” ë°ì— ì¤‘ìš”í•œ ì—­í• ì„ í–ˆë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë¦¬ê³  U-Netì˜ inductive bias(ê³ ì¸µë¶€ì˜ ë ˆì´ì–´ì—ì„œëŠ” ë””í…Œì¼í•œ ì†ì„±ì— ì§‘ì¤‘, ì €ì¸µë¶€ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë”ìš± coarse í•œ íŠ¹ì„±ì— ì§‘ì¤‘, ë…¼ë¬¸ì— ìì„¸íˆ ì„¤ëª…ì€ ë˜ì–´ìˆì§€ ì•ŠìŒ)ê°€ ìƒì„± í€„ë¦¬í‹°ì— í•µì‹¬ì ì¸ ë¶€ë¶„ì€ ì•„ë‹˜ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” U-Netì´ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ êµì²´ê°€ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì£¼ì—ˆê³ , ì´ë¥¼ í†µí•´ í‘œì¤€í™”ëœ transformerì˜ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•  ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤. ì´ëŠ” ì´í›„ì— ì˜ìƒ ìƒì„±(ì‹œê°„ ì¶•ì— ëŒ€í•œ attend), ì—¬ëŸ¬ ë„ë©”ì¸(cross-domain), ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°(multi-modal)ë¥¼ í™œìš©í•œ ì—°êµ¬ì—ë„ ì˜í–¥ì„ ì£¼ì—ˆë‹¤ê³  ìƒê°í•œë‹¤. ì•„í‚¤í…ì²˜ ì„¤ê³„ì— ìˆì–´ì„œ Vision Transformerì˜ ëª¨ë²” ì˜ˆì‹œë¥¼ ì˜ ë”°ë¥´ë„ë¡ í•˜ì˜€ë‹¤ê³  í•˜ë©°, ê·¸ ë•ì— ë…¼ë¬¸ ì´ë¦„ì— Scalableì´ë¼ëŠ” ë§ì„ ì¶”ê°€í•œ ê²ƒ ê°™ë‹¤.ê·¸ë¦¬ê³  ì„¸ë¶€ì ìœ¼ë¡œ Network complexity vs Sample quality ì¸¡ë©´ì—ì„œ scaling behaviorë¥¼ ë¹„êµí•´ë³´ì•˜ë‹¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  VAEì˜ latent spaceì—ì„œ í•™ìŠµëœ LDMê³¼ ë¹„êµí•˜ì˜€ë‹¤. ê²°ë¡ ì ìœ¼ë¡œëŠ” Network complexity(Gflopsë¡œ ì¸¡ì •ëœ ê°’)ì´ ë†’ì•„ì§ˆìˆ˜ë¡ Sample Quality(ìƒì„± í€„ë¦¬í‹°, FIDë¡œ ì¸¡ì •)ê°€ ì¢‹ì•„ì§„ë‹¤(FIDê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ í€„ë¦¬í‹°ë¥¼ ëœ»í•¨)ê³  í•œë‹¤. ìœ„ì˜ ì•„í‚¤í…ì²˜ ê·¸ë¦¼ì„ ë³´ë©´, LDMì²˜ëŸ¼ latent ê³µê°„ì—ì„œ ViT ì•„í‚¤í…ì²˜ë¥¼ ì°¨ìš©í•œ ë“¯ ë³´ì¸ë‹¤. ê·¸ë¦¬ê³ , Cross Attentionê³¼ Multi-Head Self-Attention ë˜í•œ ì‹¤í—˜í•´ ë³´ì•˜ìœ¼ë‚˜ adaLN-Zero ì•„í‚¤í…ì²˜ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•˜ì˜€ë‹¤ (Adaptive Instance Normalization (AdaIN)ì„ ì‚¬ìš©í•œ Style GANê³¼ ì¼ë¶€ ìœ ì‚¬í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ, ë‘ ê°€ì§€ì˜ ì°¨ì´ëŠ” Adaptive í•˜ê²Œ normalizationì„ í•˜ì§€ë§Œ, instance ì°¨ì›ì—ì„œ í•˜ëŠ”ì§€, layer ì°¨ì›ì—ì„œ í•˜ëŠ”ì§€ì´ë‹¤).ì´ ë…¼ë¬¸ì˜ â€œDiffusion formulationâ€ ë¶€ë¶„ì—ì„œëŠ” í™•ì‚° ëª¨ë¸, íŠ¹íˆ Gaussian diffusion models(ê°€ìš°ì‹œì•ˆ í™•ì‚° ëª¨ë¸)ì— ëŒ€í•œ ê¸°ë³¸ ê°œë…ê³¼ ìˆ˜í•™ì  ì ‘ê·¼ì„ ì„¤ëª…í•˜ê³  ìˆë‹¤. ì—¬ê¸°ì—ì„œ ì„¤ëª…í•˜ëŠ” ì£¼ìš” ê°œë…ë“¤ì„ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬í•´ë³´ì•˜ë‹¤.ë…¼ë¬¸ì˜ ì„¹ì…˜ 3ì—ì„œëŠ” ì¡°ê±´ë¶€ í™•ì‚° ëª¨ë¸(conditional diffusion models)ì—ì„œì˜ í–¥ìƒëœ ìƒ˜í”Œë§ ì ˆì°¨ë¥¼ ì„¤ëª…í•œë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ íŠ¹ì • í´ë˜ìŠ¤ ë¼ë²¨ $c$ì™€ ê°™ì€ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, ì´ëŠ” ì—­ ê³¼ì • $p_\theta(x_{t-1}|x_t, c)$ì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤.ë””í“¨ì „ ëª¨ë¸ì„ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ pixel ê³µê°„ì—ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ê³ , ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤. ë”°ë¼ì„œ, LDMì—ì„œëŠ” 2ê°€ì§€ stageë¡œ ì´ë¥¼ ê·¹ë³µí•˜ì˜€ë‹¤.Paperì—ì„œ ì–¸ê¸‰ì€ ëª…í™•íˆ í•˜ì§€ ì•Šì•˜ì§€ë§Œ, U-Net ì•„í‚¤í…ì²˜ë¥¼ ì¶”ê°€ë¡œ ì±„íƒí•˜ì—¬, ë”ìš± ì €ì°¨ì›ì—ì„œë„ í•™ìŠµì„ ì§„í–‰í•œë‹¤.ì´ ë…¼ë¬¸ì˜ ê¸°ìˆ ì ì¸ ì„¤ëª…ë“¤ì„ ì¢…í•©í•´ ë³´ë©´, off-the-shelfì˜ Convolutional VAEë¥¼ í™œìš©í•˜ëŠ” Transformer-based DDPMì´ë¼ê³  ìš”ì•½í•  ìˆ˜ ìˆë‹¤.DiTë¥¼ ì„¤ê³„í•  ë•Œ, í‘œì¤€ì ì¸ ViTì˜ scaling propertyë“¤ì„ ìœ ì§€í•˜ê³ ì í–ˆë‹¤ê³  í•œë‹¤. ë”°ë¼ì„œ, ViTì˜ ëª¨ë²” ì‚¬ë¡€ë“¤ì„ ì˜ ìœ ì§€í•˜ê³  ìˆë‹¤ê³  í•˜ë©°, ì´ ì±•í„°ì—ì„œëŠ” patchify, DiT Block design, Model size, Transformer Decoderì— ëŒ€í•´ì„œ ìì„¸íˆ ì„¤ëª…í•œë‹¤.Patchifyì— ê´€í•œ ì„¤ëª…</p>]]></content><author><name></name></author><summary type="html"><![CDATA[ë…¼ë¬¸ ì˜ ë‚´ìš©ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.ë…¼ë¬¸ ì¶œì²˜: ì•„ì¹´ì´ë¸Œ, Codeì´ ë…¼ë¬¸ì€ Facebook Research(í˜„ Meta AI)ì—ì„œ ê³µê°œí•œ ë…¼ë¬¸ì´ë©°, ì €ì ì¤‘ William PeeblesëŠ” OpenAIì—ì„œ Research Scientistë¡œ ì¼í•˜ê³  ìˆìœ¼ë©°, ì˜ìƒ ìƒì„± AI Soraì˜ ê°œë°œì„ ê³µë™ìœ¼ë¡œ ë¦¬ë“œí•˜ê³  ìˆë‹¤ê³  í•œë‹¤.DiT ì•„í‚¤í…ì²˜ì— ê´€í•œ ê´€ì‹¬ì€ OpenAIì˜ Sora ê³µê°œ ì´í›„ ëœ¨ê±°ì›Œì§„ ê²ƒ ê°™ìœ¼ë©°, ìµœê·¼ì— ê³µê°œëœ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì¸ Stable Diffusion 3, PixArt ê³„ì—´ì˜ ëª¨ë¸ì—ë„ ì ìš©ë˜ì–´ ë”ìš± ê°ê´‘ë°›ê³  ìˆë‹¤ê³  ìƒê°í•œë‹¤.ê°œê´„ì ì¸ ë…¼ë¬¸ ìš”ì•½ê°œìš”ì´ ì—°êµ¬ëŠ” ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ì‚° ëª¨ë¸ì„ ì œì•ˆí•œë‹¤. ê¸°ì¡´ì˜ U-Net êµ¬ì¡°ë¥¼ ëŒ€ì²´í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬..]]></summary></entry></feed>