<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gogiants1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gogiants1.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-15T07:11:45+00:00</updated><id>https://gogiants1.github.io/feed.xml</id><title type="html">Hyungwook Choi</title><subtitle>The personal website of Hyungwook Choi. </subtitle><entry><title type="html">[Paper Review] Presel: Pre-Instruction Data Selection for Visual Instruction Tuning</title><link href="https://gogiants1.github.io/blog/2025/Presel/" rel="alternate" type="text/html" title="[Paper Review] Presel: Pre-Instruction Data Selection for Visual Instruction Tuning"/><published>2025-06-15T00:00:00+00:00</published><updated>2025-06-15T00:00:00+00:00</updated><id>https://gogiants1.github.io/blog/2025/Presel</id><content type="html" xml:base="https://gogiants1.github.io/blog/2025/Presel/"><![CDATA[<h1 id="presel-english">Presel (English)</h1> <p><img src="../assets/img/presel.png" alt="Overview"/></p> <h2 id="-overview-of-presel-pre-instruction-data-selection-for-vit">🔍 Overview of PreSel: Pre-Instruction Data Selection for VIT</h2> <table> <thead> <tr> <th>Step</th> <th>Purpose</th> <th>Input/Output</th> <th>Core Idea</th> </tr> </thead> <tbody> <tr> <td><strong>1. Task-Importance Estimation</strong></td> <td>Decide <em>how much to sample</em> from each task</td> <td>Input: 5% reference set $D_{\text{ref}}$ (with instructions) → Output: task weights $w(T_i)$</td> <td>(i) Fine-tune LVLM on small set → Reference model <br/> (ii) Compute <strong>IRS</strong> (Instruction Relevance Score) to assess task importance</td> </tr> <tr> <td><strong>2. Task-wise Cluster-based Selection</strong></td> <td>Decide <em>which images to sample</em> from each task</td> <td>Input: Remaining 95% images (no instructions) + $w(T_i)$</td> <td>(i) Extract features using DINOv2 <br/> (ii) k-means clustering per task <br/> (iii) Select representative images via <strong>NC</strong> (Neighbor Centrality)</td> </tr> <tr> <td><strong>3. Instruction Generation &amp; LVLM Fine-tuning</strong></td> <td>Generate instructions only for selected subset $\mathcal{D}_S$ and fine-tune</td> <td>Output: (Image, Instruction) pairs → LVLM tuning</td> <td>Save cost by generating instructions only for $\mathcal{D}_S \ll D$</td> </tr> </tbody> </table> <hr/> <h2 id="1-problem-formulation">1. Problem Formulation</h2> <p>Let:</p> \[D = \bigcup_{i=1}^{M} T_i\] <p>Where:</p> <ul> <li>$T_i$: set of unlabeled images for the $i$-th vision task (e.g., VQA, OCR)</li> <li>$D$: entire image pool</li> <li>Each image $I\in T_i$ is mapped to instruction $Y = F_i(I)$, a high-cost generation process (e.g., GPT or human annotator)</li> </ul> <h3 id="objective">Objective</h3> <p>Select a small subset $\mathcal{D}_S \subset D$ with $\mathcal{D}_S \ll D$, generate instructions only for these, and fine-tune the LVLM on the resulting pairs $(I_a, Y_a)$ to achieve near full-data performance.</p> <hr/> <h2 id="2-task-importance-estimation">2. Task-Importance Estimation</h2> <h3 id="21-reference-model">2.1 Reference Model</h3> <p>Randomly sample 5% of $D$ as $D_{\text{ref}}$ (with instruction $Y = (Q, R)$) and fine-tune LVLM for one epoch → reference model.</p> <h3 id="22-instruction-relevance-score-irs">2.2 Instruction Relevance Score (IRS)</h3> <p>Given instruction $Y = (Q, R)$, define:</p> \[\mathcal{L}_{R|Q,I} = -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|} \log P_{\theta}(t^{R}_{j} \mid I, Q, t^{R}_{&lt;j}) \tag{1}\] \[\mathcal{L}_{R|I} = -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|} \log P_{\theta}(t^{R}_{j} \mid I, t^{R}_{&lt;j}) \tag{2}\] \[\text{IRS}(I, Y) = \frac{\mathcal{L}_{R|Q,I}}{\mathcal{L}_{R|I}} \tag{3}\] <ul> <li>A <strong>lower</strong> IRS implies $Q$ significantly aids in generating $R$ → more important task.</li> <li>A <strong>higher</strong> IRS implies $Q$ does <em>not</em> help much → less important task.</li> </ul> <h3 id="23-compute-task-importance">2.3 Compute Task Importance</h3> <p>Average IRS for task $T_i$:</p> \[s(T_i) = \frac{1}{|D_{\text{ref}}^i|} \sum_{I \in T_i} \text{IRS}(I, Y) \tag{4}\] <p>Compute task weight via softmax:</p> \[w(T_i) = \frac{\exp(-s(T_i)/\tau)}{\sum_{j=1}^{M} \exp(-s(T_j)/\tau)},\quad \tau = \frac{1}{\sqrt{M}} \tag{5}\] <hr/> <h2 id="3-task-wise-cluster-based-selection">3. Task-wise Cluster-based Selection</h2> <h3 id="31-visual-feature-extraction-and-clustering">3.1 Visual Feature Extraction and Clustering</h3> <p>For all unlabeled images $I \in T_i$, extract visual features $\mathbf{v}_I$ using DINOv2 ([CLS] token), and cluster into:</p> \[\left\{ A_c^i \right\}_{c=1}^{C},\quad \text{where } C = \frac{|T_i|}{100}\] <h3 id="32-cluster-allocation">3.2 Cluster Allocation</h3> <p>For cluster $A_c^i$, compute number of samples to pick:</p> \[n_c = \left\lfloor \frac{w(T_i) \cdot |A_c^i|}{|T_i|} \cdot |\mathcal{D}_S| \right\rfloor \tag{6}\] <h3 id="33-intra-cluster-selection-neighbor-centrality-nc">3.3 Intra-Cluster Selection: Neighbor Centrality (NC)</h3> <p>Score for each image $I$ based on cosine similarity with $k$-nearest neighbors:</p> \[s_{\text{NC}}(I) = \frac{1}{k} \sum_{I_a \in \text{kNN}(I)} \text{sim}(\mathbf{v}_I, \mathbf{v}_{I_a}) \tag{7}\] <ul> <li>Higher $s_{\text{NC}}(I)$ means image is <strong>central</strong> and <strong>representative</strong>.</li> </ul> <hr/> <h2 id="4-final-assembly-and-fine-tuning">4. Final Assembly and Fine-tuning</h2> <ul> <li>Selected images across all tasks form $\mathcal{D}_S$.</li> <li>Generate instructions <strong>only for $\mathcal{D}_S$</strong>.</li> <li>Fine-tune LVLM using $(I, Y)$ pairs from $\mathcal{D}_S$.</li> </ul> <hr/> <h2 id="5-key-takeaways">5. Key Takeaways</h2> <table> <thead> <tr> <th>Insight</th> <th>Benefit</th> </tr> </thead> <tbody> <tr> <td><strong>Instruction-free selection phase</strong></td> <td>Reduces GPT or annotation cost drastically</td> </tr> <tr> <td><strong>IRS for task relevance</strong></td> <td>Captures both redundancy and informativeness</td> </tr> <tr> <td><strong>Visual feature + NC</strong></td> <td>Enables language-free, representative image selection</td> </tr> <tr> <td><strong>Lightweight pipeline</strong></td> <td>Efficient for large-scale unlabeled datasets</td> </tr> </tbody> </table> <p>This approach enables scalable, cost-effective, and high-performance data curation for visual instruction tuning in Multimodal LLMs.</p> <h1 id="presel-정리-korean">Presel 정리 (Korean)</h1> <table> <thead> <tr> <th>단계</th> <th>목적</th> <th>입력/출력</th> <th>핵심 아이디어</th> </tr> </thead> <tbody> <tr> <td><strong>1. Task-Importance Estimation</strong></td> <td>“어떤 태스크에서 얼마나 많이 뽑을까?” 결정</td> <td>5 % 참조집합 $D_{\text{ref}}$ (이미지·지시 포함) → 가중치 $w(T_i)$</td> <td>(i) LVLM을 1-epoch 소량 파인튜닝 → Reference model <br/>(ii) <strong>IRS</strong>(Instruction Relevance Score)로 태스크 중요도 측정</td> </tr> <tr> <td><strong>2. Task-wise Cluster-based Selection</strong></td> <td>각 태스크 내부에서 “무엇을 뽑을까?” 결정</td> <td>나머지 95 % 미라벨 데이터 (이미지만) + $w(T_i)$</td> <td>(i) DINOv2 특성 추출 → k-means 군집화 <br/>(ii) 군집 크기·$w(T_i)$ 기반 샘플 수 $n_c$ 산정 <br/>(iii) <strong>NC</strong>(Neighbor Centrality)로 대표 이미지 선택</td> </tr> <tr> <td><strong>3. Instruction Generation &amp; LVLM Fine-tune</strong></td> <td>최종 소규모 데이터 $\mathcal{D}_S$에 대해 지시 생성·파인튜닝</td> <td>$\mathcal{D}_S$ (이미지) → (이미지, 지시) → 파인튜닝</td> <td>비용 절감: 전체 대신 ($\mathcal{D}_S \ll D$) 만 instruction 생성</td> </tr> </tbody> </table> <hr/> <h2 id="1-문제-정의-problem-formulation">1. 문제 정의 (Problem Formulation)</h2> <ul> <li> <p><strong>풀(pool) 구성</strong></p> \[D \;=\; \bigcup_{i=1}^{M} T_i,\qquad |D| = \text{총 이미지 수}\] <p>$T_i$ : VQA, OCR 등 <strong>시각 태스크</strong>에 속하는 <em>unlabeled</em> 이미지 집합.</p> </li> <li> <p><strong>지시 생성 비용</strong>: 각 $T_i$에는 GPT API 호출·사람 라벨 등 <strong>고비용 절차</strong> $F_i(\cdot)$ 이 필요.</p> </li> <li> <p><strong>목표</strong>: $\mathcal{D}_S\subset D,\;|\mathcal{D}_S|!\ll!|D|$ 를 뽑아 지시를 생성하고 (이미지, 지시) 페어로 LVLM을 파인튜닝 → <strong>전체 파인튜닝과 유사한 성능</strong> 달성.</p> </li> </ul> <hr/> <h2 id="2-task-importance-estimation-1">2. Task-Importance Estimation</h2> <h3 id="2-1-참조-모델-구축">2-1. 참조 모델 구축</h3> <ul> <li>무작위 <strong>5 %</strong> 참조집합 $D_{\text{ref}}$을 선택, 1 epoch 파인튜닝 → <strong>Reference LVLM</strong>.</li> </ul> <h3 id="2-2-instruction-relevance-score-irs">2-2. Instruction Relevance Score (IRS)</h3> <p>각 샘플 $(I,Q,R)$ 에 대해</p> \[\mathcal{L}_{R|Q,I}= -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|}\!\log P_{\theta}\!\bigl(t^{R}_{j}\mid I,Q,t^{R}_{&lt;j}\bigr) \tag{1}\] \[\mathcal{L}_{R|I}= -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|}\!\log P_{\theta}\!\bigl(t^{R}_{j}\mid I,t^{R}_{&lt;j}\bigr) \tag{2}\] \[\textbf{IRS}(I,Y)=\frac{\mathcal{L}_{R|Q,I}}{\mathcal{L}_{R|I}} \tag{3}\] <ul> <li> <p><strong>해석</strong></p> <ul> <li>IRS↑ ⇒ 질문 $Q$가 <em>도움이 안 됨</em> → 태스크 중요도↓</li> <li>IRS↓ ⇒ $Q$ 덕분에 혼동↓ → 태스크 중요도↑</li> </ul> </li> </ul> <h3 id="2-3-태스크별-평균-irs와-가중치">2-3. 태스크별 평균 IRS와 가중치</h3> \[s(T_i)=\frac{1}{|D^{i}_{\text{ref}}|}\sum_{I\in T_i}\text{IRS}(I,Y) \tag{4}\] \[w(T_i)=\frac{\exp\!\bigl(-s(T_i)/\tau\bigr)}{\sum_{j=1}^{M}\exp\!\bigl(-s(T_j)/\tau\bigr)}, \qquad \tau=\frac{1}{\sqrt{M}} \tag{5}\] <ul> <li>$w(T_i)$ : <strong>최종 샘플 비중</strong> (태스크 중요도에 softmax 적용).</li> </ul> <hr/> <h2 id="3-task-wise-cluster-based-selection-1">3. Task-wise Cluster-based Selection</h2> <h3 id="3-1-시각-특성--군집">3-1. 시각 특성 &amp; 군집</h3> <ul> <li>모든 unlabeled $I\in T_i$ 에 대해 DINOv2 [30]의 $[\text{CLS}]$ 벡터 $\mathbf{v}_{I}$ 추출.</li> <li>$k$-means 군집: $C=\tfrac{|T_i|}{100}$ 개 → 군집 $A^{i}_{c}$ ($c=1,\dots,C$) 형성.</li> </ul> <h3 id="3-2-군집-샘플-수-결정">3-2. 군집 샘플 수 결정</h3> \[n_c=\Bigl\lfloor \frac{w(T_i)\,|A^{i}_{c}|}{|T_i|}\,|\mathcal{D}_S| \Bigr\rfloor \tag{6}\] <h3 id="3-3-intra-cluster-대표성--neighbor-centrality">3-3. Intra-Cluster 대표성 — Neighbor Centrality</h3> \[s_{\text{NC}}(I)=\frac{1}{k}\sum_{I_a\in k\text{NN}(I)}\! \operatorname{sim}\!\bigl(\mathbf{v}_{I},\mathbf{v}_{I_a}\bigr) \tag{7}\] <ul> <li>knn 이웃 평균 코사인 유사도.</li> <li>$s_{\text{NC}}$ 높을수록 <strong>군집 중심</strong> → 대표 이미지로 선택.</li> </ul> <hr/> <h2 id="4-전체-파이프라인-figure-3-해설">4. 전체 파이프라인 (Figure 3 해설)</h2> <ol> <li> <p><strong>좌측</strong>: 다태스크 이미지 풀 $D$ → 5 % 추출해 $D_{\text{ref}}$ (녹색)</p> <ul> <li>이 단계에서만 지시 생성·Reference LVLM 학습 → $w(T_i)$ 도출.</li> </ul> </li> <li> <p><strong>중앙</strong>: 나머지 95 % 는 DINOv2 특성 → 태스크별 k-means → 군집 색상(연두/갈색…).</p> </li> <li> <p><strong>우측</strong>: 각 군집 내 NC 상위 $n_c$ 이미지 선정 → $\mathcal{D}_S$ (회색 상자).</p> </li> <li> <p><strong>하단 확대</strong>: IRS 계산 과정</p> <ul> <li> <table> <tbody> <tr> <td>In/Out 토큰 시퀀스 비교로 $\mathcal{L}_{R</td> <td>I}$, $\mathcal{L}_{R</td> <td>Q,I}$ 산출.</td> </tr> </tbody> </table> </li> <li>태스크별 평균 → $w(T_i)$ 산정 후 윗단계로 피드백.</li> </ul> </li> <li> <p><strong>최종</strong>: 선택된 이미지에만 지시 생성 → LVLM 다시 파인튜닝.</p> </li> </ol> <hr/> <h2 id="5-핵심-통찰--장점">5. 핵심 통찰 &amp; 장점</h2> <table> <thead> <tr> <th>포인트</th> <th>이유/효과</th> </tr> </thead> <tbody> <tr> <td><strong>“지시 없는” 상태에서 <em>선택</em> → 지시 생성</strong></td> <td>GPT API 비용·휴먼 라벨링 비용 대폭 절감</td> </tr> <tr> <td>IRS 기반 <strong>태스크 중요도</strong></td> <td>중복·학습 난이도까지 반영해 <em>균형 잡힌</em> 서브셋 구성</td> </tr> <tr> <td><strong>시각 특성+NC</strong></td> <td>언어 정보 없이도 <em>대표성</em> 중심 샘플링 → 효율적 일반화</td> </tr> <tr> <td>경량 파이프라인 (DINOv2, k-means)</td> <td>GPU 메모리 / 연산 부담 최소화, 대규모 풀에도 적용 용이</td> </tr> </tbody> </table> <p>위 과정을 구현하면 전체 VIT 데이터의 <strong>5–10 %</strong> 만으로도 풀데이터 파인튜닝 수준에 근접한 성능을 달성하면서, 지시 생성-학습 시간을 획기적으로 단축할 수 있습니다.</p>]]></content><author><name></name></author><category term="deep-learning"/><category term="mllm"/><category term="datasets"/><category term="paper-review"/><summary type="html"><![CDATA[Presel (English)]]></summary></entry><entry><title type="html">[DiT] Scalable Diffusion Models with Transformers — Dev In Seoul</title><link href="https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers-dev-in-seoul/" rel="alternate" type="text/html" title="[DiT] Scalable Diffusion Models with Transformers — Dev In Seoul"/><published>2024-07-15T00:00:00+00:00</published><updated>2024-07-15T00:00:00+00:00</updated><id>https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers--dev-in-seoul</id><content type="html" xml:base="https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers-dev-in-seoul/"><![CDATA[<p>MathJax = { tex: {inlineMath: [[’$’, ‘$’], [’\(‘, ‘\)’]]} };</p> <p>논문 <Scalable Diffusion="" Models="" with="" Transformers="">의 내용을 정리한 글입니다.논문 출처: 아카이브, Code이 논문은 Facebook Research(현 Meta AI)에서 공개한 논문이며, 저자 중 William Peebles는 OpenAI에서 Research Scientist로 일하고 있으며, 영상 생성 AI Sora의 개발을 공동으로 리드하고 있다고 한다.DiT 아키텍처에 관한 관심은 OpenAI의 Sora 공개 이후 뜨거워진 것 같으며, 최근에 공개된 이미지 생성 모델인 Stable Diffusion 3, PixArt 계열의 모델에도 적용되어 더욱 각광받고 있다고 생각한다.이 연구는 이미지 생성을 위한 새로운 접근 방식으로 트랜스포머 기반 확산 모델을 제안한다. 기존의 U-Net 구조를 대체하는 트랜스포머 아키텍처를 사용하여, 확장성과 성능 면에서 우수한 결과를 달성하려 한다.최근 딥러닝은 다양한 분야에서 획기적인 발전을 이루었으며, 특히 트랜스포머 모델은 그 가능성을 널리 입증하고 있다. 본 논문에서는 이미지 생성 분야에서 확산 모델의 기본 구조를 트랜스포머로 전환하는 새로운 시도를 소개한다. 이러한 전환을 통해 모델의 확장성과 성능을 크게 향상할 수 있다.트랜스포머는 주로 자연어 처리 분야(NLP)에서 시작되었으나, 시간이 지남에 따라 시각 인식과 같은 다른 영역에서도 중요한 역할을 하게 되었다. 트랜스포머의 주요 강점은 대규모 데이터셋에 대한 우수한 학습 능력과 더불어 높은 확장성이다.확산 모델은 이미지 생성을 위한 강력한 방법으로 자리 잡았다. 기존 연구들은 주로 컨볼루셔널 U-Net 아키텍처(Latent Diffusion Model, LDM)에 기반하여 발전되어 왔지만, 본 연구는 트랜스포머를 사용하여 이러한 모델들을 뛰어넘고자 한다.확산 프로세스는 실제 이미지 데이터에 점진적으로 노이즈를 추가하고, 이를 역전시키는 과정을 모델링한다. 본 연구에서는 이러한 과정을 트랜스포머 아키텍처를 통해 수행한다.트랜스포머를 사용한 확산 모델의 핵심은 이미지를 패치로 분할하고 각 패치를 독립적인 토큰으로 처리하는 것이다. 이 토큰들은 트랜스포머의 인코더와 디코더를 통해 처리되며, 최종적으로 노이즈가 제거된 이미지를 생성한다.DiT 모델은 다양한 구성과 패치 크기를 가지고 ImageNet 데이터셋에서 훈련되었다. 모델 성능은 FID(Fréchet Inception Distance)를 통해 평가되었다.논문의 성능 비교 그래프</Scalable></p> <p>다양한 구성의 DiT 모델들이 훈련을 거치며 FID 점수가 개선되었음을 확인할 수 있었다. FID(Fréchet Inception Distance)는 생성된 영상이나 이미지의 품질을 평가하는데 자주 사용된다. 좌측의 그래프에서는 모델의 복잡도가 높아질수록, FID는 낮아져서 생성 품질이 좋아진다는 것을 설명한다. 우측의 그림은 기존의 U-Net 기반의 Diffusion Model과의 연산량을 비교하여 효율적이라는 것을 주장하는 그래프이다. 이는 트랜스포머 기반 구조가 기존 확산 모델을 뛰어넘는 성능을 달성할 수 있음을 의미한다.본 논문에서 제안한 트랜스포머 기반 확산 모델은 높은 확장성과 우수한 성능을 제공한다. 앞으로 이 모델을 텍스트-이미지 생성 모델 등 다양한 분야에 적용할 수 있는 가능성을 탐구할 예정이다.이 논문에서는 Diffusion 기반의 생성 모델링에서, 기존 U-Net 기반의 LDM 외에 미래로 나아갈 방향을 탐색해 보는 데에 중요한 역할을 했다고 생각한다. 그리고 U-Net의 inductive bias(고층부의 레이어에서는 디테일한 속성에 집중, 저층부에 가까울수록 더욱 coarse 한 특성에 집중, 논문에 자세히 설명은 되어있지 않음)가 생성 퀄리티에 핵심적인 부분은 아님을 보여주었다. 논문에서는 U-Net이 트랜스포머로 교체가 가능함을 보여주었고, 이를 통해 표준화된 transformer의 아키텍처를 도입할 수 있었다고 한다. 이는 이후에 영상 생성(시간 축에 대한 attend), 여러 도메인(cross-domain), 여러 모달리티(multi-modal)를 활용한 연구에도 영향을 주었다고 생각한다. 아키텍처 설계에 있어서 Vision Transformer의 모범 예시를 잘 따르도록 하였다고 하며, 그 덕에 논문 이름에 Scalable이라는 말을 추가한 것 같다.그리고 세부적으로 Network complexity vs Sample quality 측면에서 scaling behavior를 비교해보았다고 한다. 그리고 VAE의 latent space에서 학습된 LDM과 비교하였다. 결론적으로는 Network complexity(Gflops로 측정된 값)이 높아질수록 Sample Quality(생성 퀄리티, FID로 측정)가 좋아진다(FID가 낮을수록 좋은 퀄리티를 뜻함)고 한다. 위의 아키텍처 그림을 보면, LDM처럼 latent 공간에서 ViT 아키텍처를 차용한 듯 보인다. 그리고, Cross Attention과 Multi-Head Self-Attention 또한 실험해 보았으나 adaLN-Zero 아키텍처를 최종적으로 선택하였다 (Adaptive Instance Normalization (AdaIN)을 사용한 Style GAN과 일부 유사하다고 볼 수 있음, 두 가지의 차이는 Adaptive 하게 normalization을 하지만, instance 차원에서 하는지, layer 차원에서 하는지이다).이 논문의 “Diffusion formulation” 부분에서는 확산 모델, 특히 Gaussian diffusion models(가우시안 확산 모델)에 대한 기본 개념과 수학적 접근을 설명하고 있다. 여기에서 설명하는 주요 개념들을 간단하게 정리해보았다.논문의 섹션 3에서는 조건부 확산 모델(conditional diffusion models)에서의 향상된 샘플링 절차를 설명한다. 이 접근 방식은 특정 클래스 라벨 $c$와 같은 추가적인 정보를 입력으로 사용하며, 이는 역 과정 $p_\theta(x_{t-1}|x_t, c)$에 영향을 미치게 된다.디퓨전 모델을 고해상도의 이미지 pixel 공간에서 학습하는 것은 비효율적이고, 비용이 많이 든다. 따라서, LDM에서는 2가지 stage로 이를 극복하였다.Paper에서 언급은 명확히 하지 않았지만, U-Net 아키텍처를 추가로 채택하여, 더욱 저차원에서도 학습을 진행한다.이 논문의 기술적인 설명들을 종합해 보면, off-the-shelf의 Convolutional VAE를 활용하는 Transformer-based DDPM이라고 요약할 수 있다.DiT를 설계할 때, 표준적인 ViT의 scaling property들을 유지하고자 했다고 한다. 따라서, ViT의 모범 사례들을 잘 유지하고 있다고 하며, 이 챕터에서는 patchify, DiT Block design, Model size, Transformer Decoder에 대해서 자세히 설명한다.Patchify에 관한 설명</p>]]></content><author><name></name></author><summary type="html"><![CDATA[논문 의 내용을 정리한 글입니다.논문 출처: 아카이브, Code이 논문은 Facebook Research(현 Meta AI)에서 공개한 논문이며, 저자 중 William Peebles는 OpenAI에서 Research Scientist로 일하고 있으며, 영상 생성 AI Sora의 개발을 공동으로 리드하고 있다고 한다.DiT 아키텍처에 관한 관심은 OpenAI의 Sora 공개 이후 뜨거워진 것 같으며, 최근에 공개된 이미지 생성 모델인 Stable Diffusion 3, PixArt 계열의 모델에도 적용되어 더욱 각광받고 있다고 생각한다.개괄적인 논문 요약개요이 연구는 이미지 생성을 위한 새로운 접근 방식으로 트랜스포머 기반 확산 모델을 제안한다. 기존의 U-Net 구조를 대체하는 트랜스포머 아키텍처를 사..]]></summary></entry></feed>