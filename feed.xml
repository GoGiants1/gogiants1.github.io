<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en, ko"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gogiants1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gogiants1.github.io/" rel="alternate" type="text/html" hreflang="en, ko"/><updated>2025-06-15T05:11:56+00:00</updated><id>https://gogiants1.github.io/feed.xml</id><title type="html">Hyungwook Choi</title><subtitle>The personal website of Hyungwook Choi. </subtitle><entry><title type="html">[DiT] Scalable Diffusion Models with Transformers — Dev In Seoul</title><link href="https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers-dev-in-seoul/" rel="alternate" type="text/html" title="[DiT] Scalable Diffusion Models with Transformers — Dev In Seoul"/><published>2024-07-15T00:00:00+00:00</published><updated>2024-07-15T00:00:00+00:00</updated><id>https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers--dev-in-seoul</id><content type="html" xml:base="https://gogiants1.github.io/blog/2024/dit-scalable-diffusion-models-with-transformers-dev-in-seoul/"><![CDATA[<p>MathJax = { tex: {inlineMath: [[’$’, ‘$’], [’\(‘, ‘\)’]]} };</p> <p>논문 <Scalable Diffusion="" Models="" with="" Transformers="">의 내용을 정리한 글입니다.논문 출처: 아카이브, Code이 논문은 Facebook Research(현 Meta AI)에서 공개한 논문이며, 저자 중 William Peebles는 OpenAI에서 Research Scientist로 일하고 있으며, 영상 생성 AI Sora의 개발을 공동으로 리드하고 있다고 한다.DiT 아키텍처에 관한 관심은 OpenAI의 Sora 공개 이후 뜨거워진 것 같으며, 최근에 공개된 이미지 생성 모델인 Stable Diffusion 3, PixArt 계열의 모델에도 적용되어 더욱 각광받고 있다고 생각한다.이 연구는 이미지 생성을 위한 새로운 접근 방식으로 트랜스포머 기반 확산 모델을 제안한다. 기존의 U-Net 구조를 대체하는 트랜스포머 아키텍처를 사용하여, 확장성과 성능 면에서 우수한 결과를 달성하려 한다.최근 딥러닝은 다양한 분야에서 획기적인 발전을 이루었으며, 특히 트랜스포머 모델은 그 가능성을 널리 입증하고 있다. 본 논문에서는 이미지 생성 분야에서 확산 모델의 기본 구조를 트랜스포머로 전환하는 새로운 시도를 소개한다. 이러한 전환을 통해 모델의 확장성과 성능을 크게 향상할 수 있다.트랜스포머는 주로 자연어 처리 분야(NLP)에서 시작되었으나, 시간이 지남에 따라 시각 인식과 같은 다른 영역에서도 중요한 역할을 하게 되었다. 트랜스포머의 주요 강점은 대규모 데이터셋에 대한 우수한 학습 능력과 더불어 높은 확장성이다.확산 모델은 이미지 생성을 위한 강력한 방법으로 자리 잡았다. 기존 연구들은 주로 컨볼루셔널 U-Net 아키텍처(Latent Diffusion Model, LDM)에 기반하여 발전되어 왔지만, 본 연구는 트랜스포머를 사용하여 이러한 모델들을 뛰어넘고자 한다.확산 프로세스는 실제 이미지 데이터에 점진적으로 노이즈를 추가하고, 이를 역전시키는 과정을 모델링한다. 본 연구에서는 이러한 과정을 트랜스포머 아키텍처를 통해 수행한다.트랜스포머를 사용한 확산 모델의 핵심은 이미지를 패치로 분할하고 각 패치를 독립적인 토큰으로 처리하는 것이다. 이 토큰들은 트랜스포머의 인코더와 디코더를 통해 처리되며, 최종적으로 노이즈가 제거된 이미지를 생성한다.DiT 모델은 다양한 구성과 패치 크기를 가지고 ImageNet 데이터셋에서 훈련되었다. 모델 성능은 FID(Fréchet Inception Distance)를 통해 평가되었다.논문의 성능 비교 그래프</Scalable></p> <p>다양한 구성의 DiT 모델들이 훈련을 거치며 FID 점수가 개선되었음을 확인할 수 있었다. FID(Fréchet Inception Distance)는 생성된 영상이나 이미지의 품질을 평가하는데 자주 사용된다. 좌측의 그래프에서는 모델의 복잡도가 높아질수록, FID는 낮아져서 생성 품질이 좋아진다는 것을 설명한다. 우측의 그림은 기존의 U-Net 기반의 Diffusion Model과의 연산량을 비교하여 효율적이라는 것을 주장하는 그래프이다. 이는 트랜스포머 기반 구조가 기존 확산 모델을 뛰어넘는 성능을 달성할 수 있음을 의미한다.본 논문에서 제안한 트랜스포머 기반 확산 모델은 높은 확장성과 우수한 성능을 제공한다. 앞으로 이 모델을 텍스트-이미지 생성 모델 등 다양한 분야에 적용할 수 있는 가능성을 탐구할 예정이다.이 논문에서는 Diffusion 기반의 생성 모델링에서, 기존 U-Net 기반의 LDM 외에 미래로 나아갈 방향을 탐색해 보는 데에 중요한 역할을 했다고 생각한다. 그리고 U-Net의 inductive bias(고층부의 레이어에서는 디테일한 속성에 집중, 저층부에 가까울수록 더욱 coarse 한 특성에 집중, 논문에 자세히 설명은 되어있지 않음)가 생성 퀄리티에 핵심적인 부분은 아님을 보여주었다. 논문에서는 U-Net이 트랜스포머로 교체가 가능함을 보여주었고, 이를 통해 표준화된 transformer의 아키텍처를 도입할 수 있었다고 한다. 이는 이후에 영상 생성(시간 축에 대한 attend), 여러 도메인(cross-domain), 여러 모달리티(multi-modal)를 활용한 연구에도 영향을 주었다고 생각한다. 아키텍처 설계에 있어서 Vision Transformer의 모범 예시를 잘 따르도록 하였다고 하며, 그 덕에 논문 이름에 Scalable이라는 말을 추가한 것 같다.그리고 세부적으로 Network complexity vs Sample quality 측면에서 scaling behavior를 비교해보았다고 한다. 그리고 VAE의 latent space에서 학습된 LDM과 비교하였다. 결론적으로는 Network complexity(Gflops로 측정된 값)이 높아질수록 Sample Quality(생성 퀄리티, FID로 측정)가 좋아진다(FID가 낮을수록 좋은 퀄리티를 뜻함)고 한다. 위의 아키텍처 그림을 보면, LDM처럼 latent 공간에서 ViT 아키텍처를 차용한 듯 보인다. 그리고, Cross Attention과 Multi-Head Self-Attention 또한 실험해 보았으나 adaLN-Zero 아키텍처를 최종적으로 선택하였다 (Adaptive Instance Normalization (AdaIN)을 사용한 Style GAN과 일부 유사하다고 볼 수 있음, 두 가지의 차이는 Adaptive 하게 normalization을 하지만, instance 차원에서 하는지, layer 차원에서 하는지이다).이 논문의 “Diffusion formulation” 부분에서는 확산 모델, 특히 Gaussian diffusion models(가우시안 확산 모델)에 대한 기본 개념과 수학적 접근을 설명하고 있다. 여기에서 설명하는 주요 개념들을 간단하게 정리해보았다.논문의 섹션 3에서는 조건부 확산 모델(conditional diffusion models)에서의 향상된 샘플링 절차를 설명한다. 이 접근 방식은 특정 클래스 라벨 $c$와 같은 추가적인 정보를 입력으로 사용하며, 이는 역 과정 $p_\theta(x_{t-1}|x_t, c)$에 영향을 미치게 된다.디퓨전 모델을 고해상도의 이미지 pixel 공간에서 학습하는 것은 비효율적이고, 비용이 많이 든다. 따라서, LDM에서는 2가지 stage로 이를 극복하였다.Paper에서 언급은 명확히 하지 않았지만, U-Net 아키텍처를 추가로 채택하여, 더욱 저차원에서도 학습을 진행한다.이 논문의 기술적인 설명들을 종합해 보면, off-the-shelf의 Convolutional VAE를 활용하는 Transformer-based DDPM이라고 요약할 수 있다.DiT를 설계할 때, 표준적인 ViT의 scaling property들을 유지하고자 했다고 한다. 따라서, ViT의 모범 사례들을 잘 유지하고 있다고 하며, 이 챕터에서는 patchify, DiT Block design, Model size, Transformer Decoder에 대해서 자세히 설명한다.Patchify에 관한 설명</p>]]></content><author><name></name></author><summary type="html"><![CDATA[논문 의 내용을 정리한 글입니다.논문 출처: 아카이브, Code이 논문은 Facebook Research(현 Meta AI)에서 공개한 논문이며, 저자 중 William Peebles는 OpenAI에서 Research Scientist로 일하고 있으며, 영상 생성 AI Sora의 개발을 공동으로 리드하고 있다고 한다.DiT 아키텍처에 관한 관심은 OpenAI의 Sora 공개 이후 뜨거워진 것 같으며, 최근에 공개된 이미지 생성 모델인 Stable Diffusion 3, PixArt 계열의 모델에도 적용되어 더욱 각광받고 있다고 생각한다.개괄적인 논문 요약개요이 연구는 이미지 생성을 위한 새로운 접근 방식으로 트랜스포머 기반 확산 모델을 제안한다. 기존의 U-Net 구조를 대체하는 트랜스포머 아키텍처를 사..]]></summary></entry></feed>