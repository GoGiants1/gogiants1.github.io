---
layout: post
title: "[Paper Review] Presel: Pre-Instruction Data Selection for Visual Instruction Tuning"
date: 2025-06-15
categories: deep learning
tags: mllm datasets paper-review
toc:
  sidebar: right
---

# Presel (English)

## ğŸ” Overview of PreSel: Pre-Instruction Data Selection for VIT

| Step                                             | Purpose                                                                      | Input/Output                                                                                 | Core Idea                                                                                                                                        |
| ------------------------------------------------ | ---------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| **1. Task-Importance Estimation**                | Decide _how much to sample_ from each task                                   | Input: 5% reference set $D_{\text{ref}}$ (with instructions) â†’ Output: task weights $w(T_i)$ | (i) Fine-tune LVLM on small set â†’ Reference model <br> (ii) Compute **IRS** (Instruction Relevance Score) to assess task importance              |
| **2. Task-wise Cluster-based Selection**         | Decide _which images to sample_ from each task                               | Input: Remaining 95% images (no instructions) + $w(T_i)$                                     | (i) Extract features using DINOv2 <br> (ii) k-means clustering per task <br> (iii) Select representative images via **NC** (Neighbor Centrality) |
| **3. Instruction Generation & LVLM Fine-tuning** | Generate instructions only for selected subset $\mathcal{D}_S$ and fine-tune | Output: (Image, Instruction) pairs â†’ LVLM tuning                                             | Save cost by generating instructions only for $\mathcal{D}\_S \ll D$                                                                             |

---

## 1. Problem Formulation

Let:

$$
D = \bigcup_{i=1}^{M} T_i
$$

Where:

- $T_i$: set of unlabeled images for the $i$-th vision task (e.g., VQA, OCR)
- $D$: entire image pool
- Each image $I\in T_i$ is mapped to instruction $Y = F_i(I)$, a high-cost generation process (e.g., GPT or human annotator)

### Objective

Select a small subset $\mathcal{D}_S \subset D$ with $|\mathcal{D}_S| \ll |D|$ and generate instructions only for these, and fine-tune the LVLM on the resulting pairs $(I_a, Y_a)$ to achieve near full-data performance.

---

## 2. Task-Importance Estimation

### 2.1 Reference Model

Randomly sample 5% of $D$ as $D_{\text{ref}}$ (with instruction $Y = (Q, R)$) and fine-tune LVLM for one epoch â†’ reference model.

### 2.2 Instruction Relevance Score (IRS)

Given instruction $Y = (Q, R)$, define:

$$
\mathcal{L}_{R|Q,I} = -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|} \log P_{\theta}(t^{R}_{j} \mid I, Q, t^{R}_{<j}) \tag{1}
$$

$$
\mathcal{L}_{R|I} = -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|} \log P_{\theta}(t^{R}_{j} \mid I, t^{R}_{<j}) \tag{2}
$$

$$
\text{IRS}(I, Y) = \frac{\mathcal{L}_{R|Q,I}}{\mathcal{L}_{R|I}} \tag{3}
$$

- A **lower** IRS implies $Q$ significantly aids in generating $R$ â†’ more important task.
- A **higher** IRS implies $Q$ does _not_ help much â†’ less important task.

### 2.3 Compute Task Importance

Average IRS for task $T_i$:

$$
s(T_i) = \frac{1}{|D_{\text{ref}}^i|} \sum_{I \in T_i} \text{IRS}(I, Y) \tag{4}
$$

Compute task weight via softmax:

$$
w(T_i) = \frac{\exp(-s(T_i)/\tau)}{\sum_{j=1}^{M} \exp(-s(T_j)/\tau)},\quad \tau = \frac{1}{\sqrt{M}} \tag{5}
$$

---

## 3. Task-wise Cluster-based Selection

### 3.1 Visual Feature Extraction and Clustering

For all unlabeled images $I \in T_i$, extract visual features $\mathbf{v}_I$ using DINOv2 (\[CLS] token), and cluster into:

$$
\left\{ A_c^i \right\}_{c=1}^{C},\quad \text{where } C = \frac{|T_i|}{100}
$$

### 3.2 Cluster Allocation

For cluster $A_c^i$, compute number of samples to pick:

$$
n_c = \left\lfloor \frac{w(T_i) \cdot |A_c^i|}{|T_i|} \cdot |\mathcal{D}_S| \right\rfloor \tag{6}
$$

### 3.3 Intra-Cluster Selection: Neighbor Centrality (NC)

Score for each image $I$ based on cosine similarity with $k$-nearest neighbors:

$$
s_{\text{NC}}(I) = \frac{1}{k} \sum_{I_a \in \text{kNN}(I)} \text{sim}(\mathbf{v}_I, \mathbf{v}_{I_a}) \tag{7}
$$

- Higher $s_{\text{NC}}(I)$ means image is **central** and **representative**.

---

## 4. Final Assembly and Fine-tuning

- Selected images across all tasks form $\mathcal{D}_S$.
- Generate instructions **only for $\mathcal{D}_S$**.
- Fine-tune LVLM using $(I, Y)$ pairs from $\mathcal{D}_S$.

---

## 5. Key Takeaways

| Insight                              | Benefit                                               |
| ------------------------------------ | ----------------------------------------------------- |
| **Instruction-free selection phase** | Reduces GPT or annotation cost drastically            |
| **IRS for task relevance**           | Captures both redundancy and informativeness          |
| **Visual feature + NC**              | Enables language-free, representative image selection |
| **Lightweight pipeline**             | Efficient for large-scale unlabeled datasets          |

This approach enables scalable, cost-effective, and high-performance data curation for visual instruction tuning in Multimodal LLMs.

# Presel ì •ë¦¬ (Korean)

| ë‹¨ê³„                                           | ëª©ì                                                          | ì…ë ¥/ì¶œë ¥                                                          | í•µì‹¬ ì•„ì´ë””ì–´                                                                                                                                      |
| ---------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Task-Importance Estimation**              | â€œì–´ë–¤ íƒœìŠ¤í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë§ì´ ë½‘ì„ê¹Œ?â€ ê²°ì •                   | 5 % ì°¸ì¡°ì§‘í•© $D_{\text{ref}}$ (ì´ë¯¸ì§€Â·ì§€ì‹œ í¬í•¨) â†’ ê°€ì¤‘ì¹˜ $w(T_i)$ | (i) LVLMì„ 1-epoch ì†ŒëŸ‰ íŒŒì¸íŠœë‹ â†’ Reference model <br>(ii) **IRS**(Instruction Relevance Score)ë¡œ íƒœìŠ¤í¬ ì¤‘ìš”ë„ ì¸¡ì •                              |
| **2. Task-wise Cluster-based Selection**       | ê° íƒœìŠ¤í¬ ë‚´ë¶€ì—ì„œ â€œë¬´ì—‡ì„ ë½‘ì„ê¹Œ?â€ ê²°ì •                     | ë‚˜ë¨¸ì§€ 95 % ë¯¸ë¼ë²¨ ë°ì´í„° (ì´ë¯¸ì§€ë§Œ) + $w(T_i)$                    | (i) DINOv2 íŠ¹ì„± ì¶”ì¶œ â†’ k-means êµ°ì§‘í™” <br>(ii) êµ°ì§‘ í¬ê¸°Â·$w(T_i)$ ê¸°ë°˜ ìƒ˜í”Œ ìˆ˜ $n_c$ ì‚°ì • <br>(iii) **NC**(Neighbor Centrality)ë¡œ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ |
| **3. Instruction Generation & LVLM Fine-tune** | ìµœì¢… ì†Œê·œëª¨ ë°ì´í„° $\mathcal{D}_S$ì— ëŒ€í•´ ì§€ì‹œ ìƒì„±Â·íŒŒì¸íŠœë‹ | $\mathcal{D}_S$ (ì´ë¯¸ì§€) â†’ (ì´ë¯¸ì§€, ì§€ì‹œ) â†’ íŒŒì¸íŠœë‹               | ë¹„ìš© ì ˆê°: ì „ì²´ ëŒ€ì‹  ($\mathcal{D}\_S \ll D$) ë§Œ instruction ìƒì„±                                                                                  |

---

## 1. ë¬¸ì œ ì •ì˜ (Problem Formulation)

- **í’€(pool) êµ¬ì„±**

  $$
  D \;=\; \bigcup_{i=1}^{M} T_i,\qquad |D| = \text{ì´ ì´ë¯¸ì§€ ìˆ˜}
  $$

  $T_i$ : VQA, OCR ë“± **ì‹œê° íƒœìŠ¤í¬**ì— ì†í•˜ëŠ” _unlabeled_ ì´ë¯¸ì§€ ì§‘í•©.

- **ì§€ì‹œ ìƒì„± ë¹„ìš©**: ê° $T_i$ì—ëŠ” GPT API í˜¸ì¶œÂ·ì‚¬ëŒ ë¼ë²¨ ë“± **ê³ ë¹„ìš© ì ˆì°¨** $F_i(\cdot)$ ì´ í•„ìš”.

- **ëª©í‘œ**: $\mathcal{D}_S\subset D,\;|\mathcal{D}_S|\!\ll\!|D|$ ë¥¼ ë½‘ì•„ ì§€ì‹œë¥¼ ìƒì„±í•˜ê³ 
  (ì´ë¯¸ì§€, ì§€ì‹œ) í˜ì–´ë¡œ LVLMì„ íŒŒì¸íŠœë‹ â†’ **ì „ì²´ íŒŒì¸íŠœë‹ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥** ë‹¬ì„±.

---

## 2. Task-Importance Estimation

### 2-1. ì°¸ì¡° ëª¨ë¸ êµ¬ì¶•

- ë¬´ì‘ìœ„ **5 %** ì°¸ì¡°ì§‘í•© $D_{\text{ref}}$ì„ ì„ íƒ, 1 epoch íŒŒì¸íŠœë‹ â†’ **Reference LVLM**.

### 2-2. Instruction Relevance Score (IRS)

ê° ìƒ˜í”Œ $(I,Q,R)$ ì— ëŒ€í•´

$$
\mathcal{L}_{R|Q,I}= -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|}\!\log P_{\theta}\!\bigl(t^{R}_{j}\mid I,Q,t^{R}_{<j}\bigr) \tag{1}
$$

$$
\mathcal{L}_{R|I}= -\frac{1}{|t^{R}|}\sum_{j=1}^{|t^{R}|}\!\log P_{\theta}\!\bigl(t^{R}_{j}\mid I,t^{R}_{<j}\bigr) \tag{2}
$$

$$
\textbf{IRS}(I,Y)=\frac{\mathcal{L}_{R|Q,I}}{\mathcal{L}_{R|I}} \tag{3}
$$

- **í•´ì„**

  - IRSâ†‘ â‡’ ì§ˆë¬¸ $Q$ê°€ _ë„ì›€ì´ ì•ˆ ë¨_ â†’ íƒœìŠ¤í¬ ì¤‘ìš”ë„â†“
  - IRSâ†“ â‡’ $Q$ ë•ë¶„ì— í˜¼ë™â†“ â†’ íƒœìŠ¤í¬ ì¤‘ìš”ë„â†‘

### 2-3. íƒœìŠ¤í¬ë³„ í‰ê·  IRSì™€ ê°€ì¤‘ì¹˜

$$
s(T_i)=\frac{1}{|D^{i}_{\text{ref}}|}\sum_{I\in T_i}\text{IRS}(I,Y) \tag{4}
$$

$$
w(T_i)=\frac{\exp\!\bigl(-s(T_i)/\tau\bigr)}{\sum_{j=1}^{M}\exp\!\bigl(-s(T_j)/\tau\bigr)}, \qquad
\tau=\frac{1}{\sqrt{M}} \tag{5}
$$

- $w(T_i)$ : **ìµœì¢… ìƒ˜í”Œ ë¹„ì¤‘** (íƒœìŠ¤í¬ ì¤‘ìš”ë„ì— softmax ì ìš©).

---

## 3. Task-wise Cluster-based Selection

### 3-1. ì‹œê° íŠ¹ì„± & êµ°ì§‘

- ëª¨ë“  unlabeled $I\in T_i$ ì— ëŒ€í•´ DINOv2 \[30]ì˜ $[\text{CLS}]$ ë²¡í„° $\mathbf{v}_{I}$ ì¶”ì¶œ.
- $k$-means êµ°ì§‘: $C=\tfrac{|T_i|}{100}$ ê°œ
  â†’ êµ°ì§‘ $A^{i}_{c}$ ($c=1,\dots,C$) í˜•ì„±.

### 3-2. êµ°ì§‘ ìƒ˜í”Œ ìˆ˜ ê²°ì •

$$
n_c=\Bigl\lfloor \frac{w(T_i)\,|A^{i}_{c}|}{|T_i|}\,|\mathcal{D}_S| \Bigr\rfloor \tag{6}
$$

### 3-3. Intra-Cluster ëŒ€í‘œì„± â€” Neighbor Centrality

$$
s_{\text{NC}}(I)=\frac{1}{k}\sum_{I_a\in k\text{NN}(I)}\! \operatorname{sim}\!\bigl(\mathbf{v}_{I},\mathbf{v}_{I_a}\bigr) \tag{7}
$$

- k-ìµœê·¼ì ‘ ì´ì›ƒ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„.
- $s_{\text{NC}}$ ë†’ì„ìˆ˜ë¡ **êµ°ì§‘ ì¤‘ì‹¬** â†’ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì„ íƒ.

---

## 4. ì „ì²´ íŒŒì´í”„ë¼ì¸ (Figure 3 í•´ì„¤)

1. **ì¢Œì¸¡**: ë‹¤íƒœìŠ¤í¬ ì´ë¯¸ì§€ í’€ $D$ â†’ 5 % ì¶”ì¶œí•´ $D_{\text{ref}}$ (ë…¹ìƒ‰)

   - ì´ ë‹¨ê³„ì—ì„œë§Œ ì§€ì‹œ ìƒì„±Â·Reference LVLM í•™ìŠµ â†’ $w(T_i)$ ë„ì¶œ.

2. **ì¤‘ì•™**: ë‚˜ë¨¸ì§€ 95 % ëŠ” DINOv2 íŠ¹ì„± â†’ íƒœìŠ¤í¬ë³„ k-means â†’ êµ°ì§‘ ìƒ‰ìƒ(ì—°ë‘/ê°ˆìƒ‰â€¦).

3. **ìš°ì¸¡**: ê° êµ°ì§‘ ë‚´ NC ìƒìœ„ $n_c$ ì´ë¯¸ì§€ ì„ ì • â†’ $\mathcal{D}_S$ (íšŒìƒ‰ ìƒì).

4. **í•˜ë‹¨ í™•ëŒ€**: IRS ê³„ì‚° ê³¼ì •

   - In/Out í† í° ì‹œí€€ìŠ¤ ë¹„êµë¡œ $\mathcal{L}_{R|I}$, $\mathcal{L}_{R|Q,I}$ ì‚°ì¶œ.
   - íƒœìŠ¤í¬ë³„ í‰ê·  â†’ $w(T_i)$ ì‚°ì • í›„ ìœ—ë‹¨ê³„ë¡œ í”¼ë“œë°±.

5. **ìµœì¢…**: ì„ íƒëœ ì´ë¯¸ì§€ì—ë§Œ ì§€ì‹œ ìƒì„± â†’ LVLM ì¬íŒŒì¸íŠœë‹.

---

## 5. í•µì‹¬ í†µì°° & ì¥ì 

| í¬ì¸íŠ¸                                      | ì´ìœ /íš¨ê³¼                                              |
| ------------------------------------------- | ------------------------------------------------------ |
| **â€œì§€ì‹œ ì—†ëŠ”â€ ìƒíƒœì—ì„œ _ì„ íƒ_ â†’ ì§€ì‹œ ìƒì„±** | GPT API ë¹„ìš©Â·íœ´ë¨¼ ë¼ë²¨ë§ ë¹„ìš© ëŒ€í­ ì ˆê°                |
| IRS ê¸°ë°˜ **íƒœìŠ¤í¬ ì¤‘ìš”ë„**                  | ì¤‘ë³µÂ·í•™ìŠµ ë‚œì´ë„ê¹Œì§€ ë°˜ì˜í•´ _ê· í˜• ì¡íŒ_ ì„œë¸Œì…‹ êµ¬ì„±    |
| **ì‹œê° íŠ¹ì„±+NC**                            | ì–¸ì–´ ì •ë³´ ì—†ì´ë„ _ëŒ€í‘œì„±_ ì¤‘ì‹¬ ìƒ˜í”Œë§ â†’ íš¨ìœ¨ì  ì¼ë°˜í™”  |
| ê²½ëŸ‰ íŒŒì´í”„ë¼ì¸ (DINOv2, k-means)           | GPU ë©”ëª¨ë¦¬ / ì—°ì‚° ë¶€ë‹´ ìµœì†Œí™”, ëŒ€ê·œëª¨ í’€ì—ë„ ì ìš© ìš©ì´ |

ìœ„ ê³¼ì •ì„ êµ¬í˜„í•˜ë©´ ì „ì²´ VIT ë°ì´í„°ì˜ **5â€“10 %** ë§Œìœ¼ë¡œë„ í’€ë°ì´í„° íŒŒì¸íŠœë‹ ìˆ˜ì¤€ì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œ, ì§€ì‹œ ìƒì„±-í•™ìŠµ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
